# hybrid_search.py
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"

import json
import re
from bm_search import bm25_search
from dense_search import dense_search

def hybrid_search(query: str, top_k: int = 10, k: int = 60):
    """
    ä½¿ç”¨ RRF (å€’é›·èåˆ) è¿›è¡Œæ··åˆæ£€ç´¢ã€‚
    å…¬å¼: Score = 1 / (k + rank)
    
    :param k: RRF å¸¸æ•°ï¼Œé€šå¸¸è®¾ä¸º 60ã€‚
    """
    
    # 1. å¹¶è¡Œè·å–ç»“æœ (é€šå¸¸å–æ¯”æœ€ç»ˆ top_k æ›´å¤šçš„å€™é€‰é›†ï¼Œæ¯”å¦‚ 50)
    candidate_k = top_k * 5 
    bm25_hits = bm25_search(query, k=candidate_k)
    dense_hits = dense_search(query, top_k=candidate_k)

    # ç”¨äºå­˜å‚¨èåˆåˆ†æ•°
    # æ ¼å¼: {docid: {"score": 0.0, "content": "...", "url": "...", "from": set()}}
    fusion_dict = {}

    # ===========================
    # 2. å¤„ç† BM25 ç»“æœ (åŸºäºæ’å)
    # ===========================
    for rank, hit in enumerate(bm25_hits):
        docid = hit["docid"]
        
        # åˆå§‹åŒ–
        if docid not in fusion_dict:
            fusion_dict[docid] = {
                "score": 0.0, 
                "content": hit.get("contents", ""), 
                "url": hit.get("url", ""), 
                "from": set()
            }
        
        # RRF ç´¯åŠ 
        fusion_dict[docid]["score"] += 1.0 / (k + rank + 1)
        fusion_dict[docid]["from"].add("bm25")

    # ===========================
    # 3. å¤„ç† Dense ç»“æœ (å…³é”®ï¼šè§£å†³ Chunk ID é—®é¢˜)
    # ===========================
    # è®°å½• Dense è¿™ä¸€ä¾§å·²ç»å¤„ç†è¿‡çš„ docidï¼Œé˜²æ­¢åŒä¸€æ–‡æ¡£çš„å¤šä¸ª chunk é‡å¤åŠ åˆ†
    # ç­–ç•¥ï¼šå¦‚æœä¸€ç¯‡æ–‡æ¡£å¤šä¸ª chunk å‘½ä¸­ï¼Œæˆ‘ä»¬åªå–æ’åæœ€é«˜çš„é‚£ä¸€æ¬¡ï¼ˆæˆ–è€…ä½ ä¹Ÿå¯ä»¥ç´¯åŠ ï¼Œä½†é€šå¸¸å–æœ€é«˜å³å¯ï¼‰
    seen_dense_docs = set()

    for rank, hit in enumerate(dense_hits):
        raw_id = hit["docid"]
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä» "doc123_chunk4" è¿˜åŸä¸º "doc123"
        real_docid = raw_id.split("_chunk")[0] 

        if real_docid in seen_dense_docs:
            continue # åŒä¸€æ–‡æ¡£çš„åç»­ chunk ä¸å†å‚ä¸æ’åè®¡ç®—ï¼ˆé¿å…é•¿æ–‡æ¡£éœ¸æ¦œï¼‰
            
        seen_dense_docs.add(real_docid)

        # åˆå§‹åŒ– (å¦‚æœ BM25 æ²¡æœåˆ°è¿™ä¸ª)
        if real_docid not in fusion_dict:
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½  dense_search è¿”å› content/url
            # å¦‚æœ dense_search æ²¡è¿”å›ï¼Œå¯èƒ½éœ€è¦å•ç‹¬æŸ¥ï¼Œæˆ–è€…åªç”¨ BM25 çš„å…ƒæ•°æ®
            fusion_dict[real_docid] = {
                "score": 0.0, 
                "content": hit.get("contents", "Denseç»“æœæš‚æ— é¢„è§ˆ"), 
                "url": hit.get("url", ""), 
                "from": set()
            }

        # RRF ç´¯åŠ 
        fusion_dict[real_docid]["score"] += 1.0 / (k + rank + 1)
        fusion_dict[real_docid]["from"].add("dense")

    # ===========================
    # 4. æ’åºä¸æ ¼å¼åŒ–
    # ===========================
    # æŒ‰ RRF åˆ†æ•°å€’åº
    sorted_docs = sorted(fusion_dict.items(), key=lambda x: x[1]["score"], reverse=True)
    
    # ===========================
    # ğŸ”¥ æ–°å¢ï¼šç»“æœå»é‡é€»è¾‘ (De-duplication)
    # ===========================
    final_results = []
    seen_identifiers = set()

    for docid, data in sorted_docs:
        url = data["url"]
        content = data["content"]
        
        # --- ç­–ç•¥ A: URL å½’ä¸€åŒ– (è§£å†³ index.htm é—®é¢˜) ---
        # 1. å»é™¤ http/https å‰ç¼€å·®å¼‚
        norm_url = url.replace("https://", "").replace("http://", "")
        # 2. å»é™¤æœ«å°¾çš„æ–œæ 
        norm_url = norm_url.rstrip("/")
        # 3. å»é™¤é»˜è®¤é¦–é¡µæ–‡ä»¶å (index.html, index.htm, default.aspx ç­‰)
        norm_url = re.sub(r'/index\.(html|htm|php|jsp)$', '', norm_url, flags=re.IGNORECASE)
        
        # --- ç­–ç•¥ B: å†…å®¹æŒ‡çº¹ (è§£å†³ URL ä¸åŒä½†å†…å®¹å®Œå…¨ä¸€æ ·çš„é—®é¢˜) ---
        # å–å‰ 50 ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼ˆä¸€èˆ¬é¦–é¡µçš„å‰ 50 å­—éƒ½æ˜¯ä¸€æ ·çš„æ ‡é¢˜ï¼‰
        # å¦‚æœä½ æƒ³æ›´ä¸¥æ ¼ï¼Œå¯ä»¥ç”¨ hashlib.md5(content.encode()).hexdigest()
        content_fingerprint = content[:50].strip()

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        # å¦‚æœ URL å½’ä¸€åŒ–åç›¸åŒï¼Œæˆ–è€…å†…å®¹æŒ‡çº¹å®Œå…¨ç›¸åŒï¼Œå°±è§†ä¸ºé‡å¤
        if norm_url in seen_identifiers:
            continue
        # if content_fingerprint in seen_identifiers: # å¯é€‰ï¼šå¦‚æœ URL ä¸åŒä½†å†…å®¹ä¸€æ ·ä¹Ÿæƒ³å»é‡ï¼Œå¼€å¯è¿™è¡Œ
        #     continue

        # è®°å½•å·²å‡ºç°çš„ç‰¹å¾
        seen_identifiers.add(norm_url)
        # seen_identifiers.add(content_fingerprint) 

        # åŠ å…¥æœ€ç»ˆç»“æœ
        final_results.append({
            "docid": docid,
            "score": data["score"],
            "url": url,       # è¿˜æ˜¯è¿”å›åŸå§‹ URL ç»™ç”¨æˆ·
            "contents": content,
            "from": list(data["from"])
        })

        if len(final_results) >= top_k:
            break

    return final_results

if __name__ == "__main__":
    q = "ä¸­å›½äººæ°‘å¤§å­¦ é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢ äººå·¥æ™ºèƒ½ ä¸“ä¸šä»‹ç»"
    
    # æ³¨æ„ï¼šRRF ä¸éœ€è¦ alpha å‚æ•°ï¼
    results = hybrid_search(q, top_k=5)
    
    print(f"\nğŸš€ æ··åˆæ£€ç´¢ç»“æœ (Top {len(results)}):")
    for i, r in enumerate(results, 1):
        print(f"[{i}] {r['docid']} (Score: {r['score']:.4f}) Sources: {r['from']}")
        print(f"    URL: {r['url']}")
        print(f"    Preview: {r['contents'][:60].replace(chr(10), ' ')}...")
        print("-" * 60)