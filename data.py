import json
import re
import time
import requests
from bs4 import BeautifulSoup
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
INPUT_FILE = "temp_urls.json"
OUTPUT_FILE = "corpus.jsonl"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# å¹¶å‘æ•°ï¼š70ä¸‡æ•°æ®å»ºè®®ä¿å®ˆç‚¹ï¼Œé¿å…è¢«å°IPå¯¼è‡´å‰åŠŸå°½å¼ƒ
MAX_WORKERS = 50
# ===========================================

def normalize_url(url):
    """URL å½’ä¸€åŒ–"""
    if not url: return ""
    u = url.strip()
    u = u.replace("https://", "").replace("http://", "")
    u = u.rstrip("/")
    u = re.sub(r'/index\.(html|htm|php|jsp|asp|aspx)$', '', u, flags=re.IGNORECASE)
    return u

def fetch_and_process(task):
    """
    å•ä¸ªä»»åŠ¡å¤„ç†
    """
    doc_id = task["id"]
    url = task["url"]
    
    target_url = url if url.startswith("http") else "http://" + url
    content = ""
    
    try:
        # timeout è®¾ç½®ä¸º 10ç§’ï¼Œé˜²æ­¢å¡æ­»
        response = requests.get(target_url, headers={"User-Agent": USER_AGENT}, timeout=10)
        
        if response.encoding == 'ISO-8859-1':
             response.encoding = response.apparent_encoding
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            # ç§»é™¤å¹²æ‰°å…ƒç´ 
            for script in soup(["script", "style", "nav", "footer", "iframe", "noscript", "svg"]):
                script.extract()
            content = soup.get_text(separator=" ", strip=True)
            
    except Exception:
        # ç½‘ç»œé”™è¯¯åœ¨å¤§é‡çˆ¬å–ä¸­å¾ˆå¸¸è§ï¼Œè®°å½•ä¸ºç©ºå†…å®¹å³å¯ï¼Œä¸è¦ä¸­æ–­ç¨‹åº
        pass

    return {
        "id": doc_id,
        "url": url,
        "contents": content
    }

def get_finished_urls():
    """
    æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ï¼Œè·å–å·²ç»çˆ¬å–è¿‡çš„ URL é›†åˆ (ç”¨äºæ–­ç‚¹ç»­çˆ¬)
    """
    finished = set()
    max_doc_id = 0
    
    if not os.path.exists(OUTPUT_FILE):
        return finished, 0

    print(f"ğŸ”„ æ£€æµ‹åˆ° {OUTPUT_FILE} å·²å­˜åœ¨ï¼Œæ­£åœ¨æ‰«ææ–­ç‚¹...")
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line: continue
                try:
                    data = json.loads(line)
                    # è®°å½•å½’ä¸€åŒ– URL
                    u = normalize_url(data.get("url", ""))
                    if u: finished.add(u)
                    
                    # è§£æ docID æ•°å­—ï¼Œä¸ºäº†è®© ID ç»§ç»­å¾€ä¸‹æ’
                    # å‡è®¾ ID æ ¼å¼ä¸º doc123
                    cid = data.get("id", "doc0").replace("doc", "")
                    if cid.isdigit():
                        cid_num = int(cid)
                        if cid_num > max_doc_id:
                            max_doc_id = cid_num
                except:
                    continue
    except Exception as e:
        print(f"è¯»å–æ–­ç‚¹æ–‡ä»¶å‡ºé”™: {e}")
        
    print(f"âœ… å·²å®Œæˆ: {len(finished)} æ¡ï¼Œæœ€å¤§ ID: doc{max_doc_id}")
    return finished, max_doc_id

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ° {INPUT_FILE}")
        return

    # 1. è·å–æ–­ç‚¹ä¿¡æ¯
    finished_urls, last_doc_num = get_finished_urls()

    # 2. è¯»å–åŸå§‹ URL åˆ—è¡¨
    print("ğŸ“– æ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶...")
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        raw_list = json.load(f)

    # 3. ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ (è¿‡æ»¤æ‰å·²å®Œæˆçš„)
    print("âš™ï¸ æ­£åœ¨ç”Ÿæˆä»»åŠ¡é˜Ÿåˆ—...")
    tasks = []
    # è¿™é‡Œçš„ unique_urls ç”¨äºå¤„ç†æœ¬æ¬¡è¾“å…¥æ–‡ä»¶å†…éƒ¨çš„é‡å¤
    # åŒæ—¶ä¹Ÿè¦å’Œ finished_urls åšæ¯”å¯¹
    seen_in_this_run = set()
    
    doc_counter = last_doc_num + 1 # ID æ¥ç€ä¸Šæ¬¡çš„ç»§ç»­

    for item in raw_list:
        raw_url = item.get("url")
        if not raw_url: continue

        norm_url = normalize_url(raw_url)
        
        # å¦‚æœå·²ç»çˆ¬è¿‡ (æ–­ç‚¹)ï¼Œæˆ–è€…æœ¬æ¬¡è¿è¡Œä»»åŠ¡é‡Œå·²ç»æœ‰äº† (å†…éƒ¨å»é‡)
        if norm_url in finished_urls or norm_url in seen_in_this_run:
            continue
        
        seen_in_this_run.add(norm_url)
        
        tasks.append({
            "id": f"doc{doc_counter}",
            "url": raw_url
        })
        doc_counter += 1

    # é‡Šæ”¾åŸå§‹åˆ—è¡¨å†…å­˜
    del raw_list 
    
    total_tasks = len(tasks)
    print(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡ï¼šè·³è¿‡ {len(finished_urls)} æ¡ï¼Œå‰©ä½™éœ€çˆ¬å– {total_tasks} æ¡")
    
    if total_tasks == 0:
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€çˆ¬å–ã€‚")
        return

    print(f"ğŸš€ å¯åŠ¨å¹¶å‘çˆ¬å– (Workers={MAX_WORKERS})ï¼Œç»“æœå°†å®æ—¶å†™å…¥æ–‡ä»¶...")

    # 4. è¾¹çˆ¬è¾¹å†™
    # ä½¿ç”¨ 'a' (append) æ¨¡å¼æ‰“å¼€æ–‡ä»¶
    with open(OUTPUT_FILE, "a", encoding="utf-8") as f_out:
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤ä»»åŠ¡
            futures = [executor.submit(fetch_and_process, task) for task in tasks]
            
            # ä½¿ç”¨ tqdm ç›‘æ§è¿›åº¦
            for future in tqdm(as_completed(futures), total=total_tasks, unit="é¡µ"):
                try:
                    result = future.result()
                    
                    # æ ¸å¿ƒä¿®æ”¹ï¼šç«‹å³å†™å…¥æ–‡ä»¶ï¼Œä¸å­˜å†…å­˜
                    line = json.dumps(result, ensure_ascii=False)
                    f_out.write(line + "\n")
                    
                    # å¯é€‰ï¼šæ¯å†™ 10 æ¡å¼ºåˆ¶åˆ·å…¥ç¡¬ç›˜ï¼Œé˜²æ­¢æ–­ç”µæ•°æ®ä¸¢å¤±å¤ªå¤š
                    # f_out.flush() 
                    
                except Exception as e:
                    print(f"å†™å…¥å¼‚å¸¸: {e}")

    print(f"\nğŸ‰ çˆ¬å–ç»“æŸï¼æ•°æ®å·²è¿½åŠ è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()