# build_dense_index.py
import os
import json
import faiss
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ========= è·¯å¾„æŒ‰ä½ çš„ç›®å½•ç»“æ„è®¾ç½® =========
CORPUS_DIR = "/Users/cik-z/Desktop/æ™ºèƒ½ä¿¡æ¯æ£€ç´¢å¯¼è®º/ä½œä¸š/final/corpus_dir"
OUTPUT_INDEX = "/Users/cik-z/Desktop/æ™ºèƒ½ä¿¡æ¯æ£€ç´¢å¯¼è®º/ä½œä¸š/final/dense_index/dense.index"
OUTPUT_IDS = "/Users/cik-z/Desktop/æ™ºèƒ½ä¿¡æ¯æ£€ç´¢å¯¼è®º/ä½œä¸š/final/dense_index/docids.json"

CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

MODEL_NAME = "BAAI/bge-small-zh-v1.5"


def chunk_text(text, size=300, overlap=50):
    """æŠŠæ–‡æœ¬åˆ‡ç‰‡æˆ chunk"""
    text = text.strip()
    chunks = []
    start = 0

    while start < len(text):
        end = min(start + size, len(text))
        chunks.append(text[start:end])
        start += size - overlap

    return chunks


def build_dense_index():
    print("åŠ è½½ Embedding æ¨¡å‹ï¼š", MODEL_NAME)
    model = SentenceTransformer(MODEL_NAME)
    emb_dim = model.get_sentence_embedding_dimension()
    print("Embedding ç»´åº¦ï¼š", emb_dim)

    print("\nå¼€å§‹è¯»å– corpus_dir ä¸‹çš„ JSONL æ–‡ä»¶...\n")

    texts = []
    ids = []

    # ===== éå† corpus_dir ä¸‹æ‰€æœ‰ jsonl æ–‡ä»¶ =====
    json_files = [f for f in os.listdir(CORPUS_DIR) if f.endswith(".jsonl")]

    for fname in json_files:
        path = os.path.join(CORPUS_DIR, fname)
        print(f"è¯»å–æ–‡ä»¶ï¼š{path}")

        # è·å–è¡Œæ•°ä»¥æ˜¾ç¤ºè¿›åº¦æ¡
        total_lines = sum(1 for _ in open(path, "r", encoding="utf-8"))

        with open(path, "r", encoding="utf-8") as f:
            for line in tqdm(f, total=total_lines, desc=f"å¤„ç† {fname}"):
                obj = json.loads(line)
                docid = obj.get("id", "")
                content = obj.get("contents", "")

                if not content or not docid:
                    continue

                chunks = chunk_text(content, CHUNK_SIZE, CHUNK_OVERLAP)
                for idx, ch in enumerate(chunks):
                    texts.append(ch)
                    ids.append(f"{docid}_chunk{idx}")

    print(f"\nğŸ“Œ æ€» chunk æ•°é‡ï¼š{len(texts)}\n")

    # ===== å¯¹æ‰€æœ‰ chunks ç¼–ç å‘é‡ =====
    print("å¼€å§‹ç¼–ç å‘é‡ï¼ˆembeddingï¼‰...\n")
    embeddings = []

    batch_size = 32
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding è¿›åº¦"):
        batch = texts[i:i + batch_size]
        vecs = model.encode(batch, normalize_embeddings=True)
        embeddings.append(vecs)

    embeddings = np.vstack(embeddings).astype("float32")

    print("\nå‘é‡ç¼–ç å®Œæˆï¼Œå¼€å§‹æ„å»º FAISS index...\n")

    # ===== æ„å»º FAISS IndexFlatIP =====
    index = faiss.IndexFlatIP(emb_dim)
    index.add(embeddings)

    os.makedirs(os.path.dirname(OUTPUT_INDEX), exist_ok=True)
    faiss.write_index(index, OUTPUT_INDEX)

    with open(OUTPUT_IDS, "w", encoding="utf-8") as f:
        json.dump(ids, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ å®Œæˆï¼")
    print(f"å‘é‡ç´¢å¼•ä¿å­˜åœ¨ï¼š{OUTPUT_INDEX}")
    print(f"chunk-ID æ˜ å°„ä¿å­˜åœ¨ï¼š{OUTPUT_IDS}")


if __name__ == "__main__":
    build_dense_index()
